# Daily Paper Summary: 2026-02-11

## Title: Attention Is All You Need
**Authors:** Ashish Vaswani, et al.

### Summary
This foundational paper introduces the Transformer architecture, which replaces recurrent and convolutional layers with self-attention mechanisms. It is the basis for most modern LLMs like GPT and Gemini.

[Read Paper](https://arxiv.org/abs/1706.03762)